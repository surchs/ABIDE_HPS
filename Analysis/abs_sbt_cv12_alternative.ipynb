{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean HPS model with cross-validated subtyping\n",
    "- take the full sample\n",
    "- subsample using 10-fold cross-validation\n",
    "- regress nuisance separately in the train and test fold\n",
    "- extract subtypes on the train data\n",
    "- extract weights for these subtypes from the train and test data\n",
    "- predict easy cases on weights in train data\n",
    "- predict ASD in test data using train data easy case signature\n",
    "- report model performance\n",
    "\n",
    "Other ideas:\n",
    "- scale train data before nuisance regression\n",
    "- then apply the fitted scaler to the test data before their nuisance regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "sys.path.append('/home/surchs/git/HPS')\n",
    "from hps.predic import high_confidence\n",
    "from hps.visu import hps_visu\n",
    "sys.path.append('/home/surchs/git/HPS/examples/')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import visu_demo\n",
    "import scipy as sp\n",
    "import patsy as pat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import sklearn as skl\n",
    "import scipy.io as sio\n",
    "import seaborn as sbn\n",
    "import sklearn.metrics as skm\n",
    "from scipy import cluster as scl\n",
    "from scipy import stats as spt\n",
    "from nilearn import plotting as nlp\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model as sln\n",
    "from sklearn import preprocessing as skp\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seed = 12\n",
    "n_subtypes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "root_p = '/home/surchs/sim_big'\n",
    "# Pheno\n",
    "sample_p = os.path.join(root_p, 'PROJECT/abide_hps/pheno', 'psm_abide1.csv')\n",
    "# Data\n",
    "ct_p = os.path.join(root_p, 'PROJECT/abide_hps/ct')\n",
    "seed_p = os.path.join(root_p, 'PROJECT/abide_hps/seed', 'MIST_{}'.format(n_seed))\n",
    "mask_p = os.path.join(root_p, 'PROJECT/abide_hps/mask', 'MIST_mask.nii.gz')\n",
    "label_p = os.path.join(root_p, 'ATLAS/MIST/Parcel_Information', 'MIST_{}.csv'.format(n_seed))\n",
    "atlas_p = os.path.join(root_p, 'ATLAS/MIST/Parcellations/', 'MIST_{}.nii.gz'.format(n_seed))\n",
    "# File templates\n",
    "ct_t = '{}+{:07}_{}+{}_native_rms_rsl_tlaplace_30mm_{}.txt'\n",
    "sd_t = 'sub_{{}}_mist_{}.npy'.format(n_seed)\n",
    "# Make a temp save of the model\n",
    "dump_p = os.path.join(root_p, 'PROJECT/abide_hps/single_dump_mist_{}.p'.format(n_seed))#_single_intercept\n",
    "avg_dump_p = os.path.join(root_p, 'PROJECT/abide_hps/avg_dump_mist_{}_prereg_avg.p'.format(n_seed))#\n",
    "out_p = os.path.join(root_p, 'PROJECT/abide_hps/ohbm_out')\n",
    "train_resid_p = os.path.join(root_p, 'PROJECT/abide_hps/train_resid_{{}}_{}.npy'.format(n_seed))\n",
    "train_sbt_p = os.path.join(root_p, 'PROJECT/abide_hps/train_sbt_{{}}_{}.npz'.format(n_seed))\n",
    "test_resid_p = os.path.join(root_p, 'PROJECT/abide_hps/test_resid_{{}}_{}.npy'.format(n_seed))\n",
    "if not os.path.isdir(out_p):\n",
    "    os.makedirs(out_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "sample = pd.read_csv(sample_p)\n",
    "sample['DX_CODE'] = sample['DX_GROUP'].replace({'Autism':1, 'Control':0})\n",
    "label = pd.read_csv(label_p, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_i = nib.load(mask_p)\n",
    "mask = mask_i.get_data().astype(bool)\n",
    "n_vox = np.sum(mask)\n",
    "atlas = nib.load(atlas_p).get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the CV model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2_coeff(A,B):\n",
    "    # Rowwise mean of input arrays & subtract from input arrays themeselves\n",
    "    A_mA = A - A.mean(1)[:,None]\n",
    "    B_mB = B - B.mean(1)[:,None]\n",
    "\n",
    "    # Sum of squares across rows\n",
    "    ssA = (A_mA**2).sum(1);\n",
    "    ssB = (B_mB**2).sum(1);\n",
    "\n",
    "    # Finally get corr coeff\n",
    "    return np.dot(A_mA,B_mB.T)/np.sqrt(np.dot(ssA[:,None],ssB[None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtype(stack, n_subtypes):\n",
    "    # Normalize and then get the distance\n",
    "    norm = skp.scale(stack, axis=1)\n",
    "    # Get the lower triangle of the distance metric\n",
    "    dist = sp.spatial.distance.pdist(norm)\n",
    "    # Build the cluster\n",
    "    link = scl.hierarchy.linkage(dist, method='ward')\n",
    "    order = scl.hierarchy.dendrogram(link, no_plot=True)['leaves']\n",
    "    part = scl.hierarchy.fcluster(link, n_subtypes, criterion='maxclust')\n",
    "    return order, part, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress_fc(sample, formula, n_vox, n_seed, seed_p, sd_t):\n",
    "    n_sub = sample.shape[0]\n",
    "    resid_seed = np.zeros((n_sub, n_vox, n_seed))\n",
    "    dmat_seed = pat.dmatrix(formula, data=sample)\n",
    "    for sid in range(n_seed):\n",
    "        # Build the regression model for the seed maps\n",
    "        mod = sln.LinearRegression(fit_intercept=False, normalize=False, n_jobs=-1)\n",
    "        sub_seed = np.zeros((n_sub, n_vox))\n",
    "        # Line index doesn't necessarily match continuous index\n",
    "        for rid, (rid_abs, row) in enumerate(sample.iterrows()):\n",
    "            p = os.path.join(seed_p, sd_t.format(row['SUB_ID']))\n",
    "            d = np.load(p)\n",
    "            sub_seed[rid, :] = d[sid, ...]\n",
    "        res = mod.fit(dmat_seed, sub_seed)\n",
    "        resid = sub_seed - res.predict(dmat_seed)\n",
    "        resid_seed[..., sid] = resid\n",
    "    \n",
    "    return resid_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress_ct(sample, formula, ct_p, ct_t):\n",
    "    n_sub = sample.shape[0]\n",
    "    # Generate the CT residuals\n",
    "    for rid, (rid_abs, row) in enumerate(sample.iterrows()):\n",
    "        p_right = os.path.join(ct_p, ct_t.format(row['Site'], row['Subject'], row['Session'], row['Run'], 'right'))\n",
    "        p_left = os.path.join(ct_p, ct_t.format(row['Site'], row['Subject'], row['Session'], row['Run'], 'left'))\n",
    "        ct_l = pd.read_csv(p_left, header=None)[0].values\n",
    "        ct_r = pd.read_csv(p_right, header=None)[0].values\n",
    "        # Combine left and right\n",
    "        ct = np.concatenate((ct_l, ct_r))\n",
    "        if rid==0:\n",
    "            n_vert = len(ct)\n",
    "            sub_ct = np.zeros((n_sub, n_vert))\n",
    "        sub_ct[rid, :] = ct\n",
    "    dmat_ct = pat.dmatrix(formula, data=sample)\n",
    "    mod = sln.LinearRegression(fit_intercept=False, normalize=False, n_jobs=-1)\n",
    "    res = mod.fit(dmat_ct, sub_ct)\n",
    "    resid_ct = sub_ct - res.predict(dmat_ct)\n",
    "    \n",
    "    return resid_ct, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subtype_fc(resid, n_subtypes=5):\n",
    "    n_sub, n_vox, n_seed = resid.shape\n",
    "    # Run the FC subtypes\n",
    "    weights_fc = np.zeros((n_sub, n_subtypes, n_seed))\n",
    "    subtypes_fc = np.zeros((n_subtypes,) + resid.shape[1:])\n",
    "    parts_fc = np.zeros((n_sub, n_seed))\n",
    "    orders_fc = np.zeros((n_sub, n_seed))\n",
    "    dists_fc = np.zeros((n_sub, n_sub, n_seed))\n",
    "\n",
    "    for sid in range(n_seed):\n",
    "        order_fc, part_fc, dist_fc = subtype(resid[..., sid], n_subtypes)\n",
    "        dists_fc[..., sid] = sp.spatial.distance.squareform(dist_fc)\n",
    "        parts_fc[:, sid] = part_fc\n",
    "        orders_fc[:, sid] = order_fc\n",
    "        # Make the subtypes\n",
    "        subtypes_fc_tmp = np.array([np.mean(resid[part_fc==i, :, sid], 0) \n",
    "                                    for i in range(1,n_subtypes+1)])\n",
    "        subtypes_fc[..., sid] = subtypes_fc_tmp\n",
    "        # Compute the weights\n",
    "        weights_fc[..., sid] = corr2_coeff(resid[..., sid], subtypes_fc_tmp)\n",
    "    return subtypes_fc, weights_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subtype_ct(resid, n_subtypes):\n",
    "    order_ct, part_ct, dist_ct = subtype(resid, n_subtypes)\n",
    "    # Make the subtypes\n",
    "    subtypes_ct = np.array([np.mean(resid[part_ct==i, :], 0) \n",
    "                            for i in range(1,n_subtypes+1)])\n",
    "    # Compute the weights\n",
    "    weights_ct = corr2_coeff(resid, subtypes_ct)\n",
    "    return (subtypes_ct, weights_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_fc(subtypes, resid):\n",
    "    n_sub, n_vox, n_seed = resid.shape\n",
    "    n_subtypes = subtypes.shape[0]\n",
    "    weights_fc = np.zeros((n_sub, n_subtypes, n_seed))\n",
    "    for sid in range(n_seed):\n",
    "    # Compute the weights\n",
    "        weights_fc[..., sid] = corr2_coeff(resid[..., sid], subtypes[..., sid])\n",
    "    return weights_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_ct(subtypes, resid):\n",
    "    weights_ct = corr2_coeff(resid, subtypes)\n",
    "    return weights_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full range of subject indices and clinical labels\n",
    "sub_indices = sample.index.values\n",
    "labels = sample['DX_CODE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_cols = ['fc_n{}_s{}'.format(nid+1, sid+1) \n",
    "           for sid in range(n_subtypes) \n",
    "           for nid in range(n_seed)]\n",
    "fc_col_params = [(nid, sid) for sid in range(n_subtypes) \n",
    "                 for nid in range(n_seed)]\n",
    "\n",
    "ct_cols = ['ct_s{}'.format(sid+1) \n",
    "           for sid in range(n_subtypes)]\n",
    "ct_col_params = [(-1, sid) for sid in range(n_subtypes)]\n",
    "cols = ct_cols + fc_cols\n",
    "#col_features = ['BV', 'AGE_AT_SCAN', 'FD_scrubbed', ] + cols\n",
    "#col_features = ['BV', 'AGE_AT_SCAN'] + cols\n",
    "col_features = cols\n",
    "#col_params = [(None, None)]*3 + ct_col_params + fc_col_params\n",
    "#col_params = [(None, None)]*2 + ct_col_params + fc_col_params\n",
    "col_params = ct_col_params + fc_col_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sbt from /home/surchs/sim_big/PROJECT/abide_hps/train_sbt_0_12.npz\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 1 done. Took 144.99s (144.99s), 144.99s total, 1304.92s to go.\n",
      "loading sbt from /home/surchs/sim_big/PROJECT/abide_hps/train_sbt_1_12.npz\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 2 done. Took 142.30s (143.64s), 287.29s total, 1149.15s to go.\n",
      "loading sbt from /home/surchs/sim_big/PROJECT/abide_hps/train_sbt_2_12.npz\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 3 done. Took 164.11s (150.47s), 451.40s total, 1053.26s to go.\n",
      "loading sbt from /home/surchs/sim_big/PROJECT/abide_hps/train_sbt_3_12.npz\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 4 done. Took 165.06s (154.11s), 616.46s total, 924.68s to go.\n",
      "loading sbt from /home/surchs/sim_big/PROJECT/abide_hps/train_sbt_4_12.npz\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 5 done. Took 151.66s (153.62s), 768.12s total, 768.12s to go.\n",
      "loading sbt from /home/surchs/sim_big/PROJECT/abide_hps/train_sbt_5_12.npz\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 6 done. Took 165.92s (155.67s), 934.03s total, 622.69s to go.\n",
      "loading sbt from /home/surchs/sim_big/PROJECT/abide_hps/train_sbt_6_12.npz\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 7 done. Took 154.35s (155.48s), 1088.38s total, 466.45s to go.\n",
      "loading sbt from /home/surchs/sim_big/PROJECT/abide_hps/train_sbt_7_12.npz\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 8 done. Took 150.00s (154.80s), 1238.38s total, 309.60s to go.\n",
      "loading sbt from /home/surchs/sim_big/PROJECT/abide_hps/train_sbt_8_12.npz\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 9 done. Took 159.60s (155.33s), 1397.98s total, 155.33s to go.\n",
      "loading sbt from /home/surchs/sim_big/PROJECT/abide_hps/train_sbt_9_12.npz\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 10 done. Took 141.79s (153.98s), 1539.77s total, 0.00s to go.\n"
     ]
    }
   ],
   "source": [
    "scores_s1_l = list()\n",
    "scores_s2_l = list()\n",
    "y_target_l = list()\n",
    "\n",
    "start = time.time()\n",
    "took = []\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for cv_idx, (train_index, test_index) in enumerate(skf.split(sub_indices, labels)):\n",
    "    cv_start = time.time()\n",
    "    \n",
    "    train_p = train_resid_p.format(cv_idx)\n",
    "    sbt_p = train_sbt_p.format(cv_idx)\n",
    "    test_p = test_resid_p.format(cv_idx)\n",
    "    \n",
    "    # Get the train, and test sample\n",
    "    train_sample = sample.loc[train_index]\n",
    "    test_sample = sample.loc[test_index]\n",
    "    n_sub_train = train_sample.shape[0]\n",
    "    n_sub_test = test_sample.shape[0]\n",
    "    \n",
    "    # Replicate the subtyping process\n",
    "    # Extract the train and test data and regress nuisance factors\n",
    "    if not os.path.isfile(train_p):\n",
    "        train_resid_fc = regress_fc(train_sample, \n",
    "                                   'AGE_AT_SCAN + FD_scrubbed + Site', \n",
    "                                   n_vox, n_seed=n_seed, \n",
    "                                   seed_p=seed_p, sd_t=sd_t)\n",
    "\n",
    "        np.save(train_p, train_resid_fc)\n",
    "    else:\n",
    "        train_resid_fc = np.load(train_p)\n",
    "        \n",
    "    if not os.path.isfile(test_p):\n",
    "        test_resid_fc = regress_fc(test_sample, \n",
    "                                  'AGE_AT_SCAN + FD_scrubbed + Site', \n",
    "                                  n_vox, n_seed=n_seed, \n",
    "                                  seed_p=seed_p, sd_t=sd_t)\n",
    "        np.save(test_p, test_resid_fc)\n",
    "    else:\n",
    "        test_resid_fc = np.load(test_p)\n",
    "        \n",
    "    (train_resid_ct, mod_ct_train) = regress_ct(train_sample, 'AGE_AT_SCAN + Site', ct_p, ct_t)\n",
    "    (test_resid_ct, mod_ct_test) = regress_ct(test_sample, 'AGE_AT_SCAN + Site', ct_p, ct_t)\n",
    "    # Make the subtypes from the train data\n",
    "    \n",
    "    \n",
    "    if not os.path.isfile(sbt_p):\n",
    "        (subtypes_fc, train_weights_fc) = make_subtype_fc(train_resid_fc, n_subtypes=n_subtypes)\n",
    "        np.savez(sbt_p, subtypes_fc=subtypes_fc, train_weights_fc=train_weights_fc)\n",
    "    else:\n",
    "        print('loading sbt from {}'.format(sbt_p))\n",
    "        tmp = np.load(sbt_p)\n",
    "        subtypes_fc = tmp['subtypes_fc']\n",
    "        train_weights_fc = tmp['train_weights_fc']\n",
    "    \n",
    "    (subtypes_ct, train_weights_ct) = make_subtype_ct(train_resid_ct, n_subtypes=n_subtypes)\n",
    "    # Get the test weights\n",
    "    test_weights_fc = make_weights_fc(subtypes_fc, test_resid_fc)\n",
    "    test_weights_ct = make_weights_ct(subtypes_ct, test_resid_ct)\n",
    "    \n",
    "    # Build input data\n",
    "    train_fc = np.reshape(train_weights_fc, (n_sub_train, n_subtypes*n_seed))\n",
    "    test_fc = np.reshape(test_weights_fc, (n_sub_test, n_subtypes*n_seed))\n",
    "    train_w = np.concatenate((train_weights_ct, train_fc), 1)\n",
    "    test_w = np.concatenate((test_weights_ct, test_fc), 1)\n",
    "    \n",
    "    # Make sure we use the correct index or else there will be NaNs in the weight columns\n",
    "    w_data_train = pd.DataFrame(data=train_w, columns=cols, index=train_index)\n",
    "    data_train = train_sample.join(w_data_train)\n",
    "    w_data_test = pd.DataFrame(data=test_w, columns=cols, index=test_index)\n",
    "    data_test = test_sample.join(w_data_test)\n",
    "    \n",
    "    # Select the features\n",
    "    scaler = skl.preprocessing.StandardScaler()\n",
    "    x_train = data_train.loc[:, col_features]\n",
    "    # Normalize\n",
    "    X_train = scaler.fit_transform(x_train)\n",
    "    # Take the numeric diagnosis code, 0 is control, 1 is autism\n",
    "    y_train = data_train.loc[:, ['DX_CODE']].values.squeeze()\n",
    "\n",
    "    # Same for the test data\n",
    "    x_test = data_test.loc[:, col_features]\n",
    "    # Normalize, but use the fitted scalar of the training data\n",
    "    X_test = scaler.transform(x_test)\n",
    "    y_test = data_test.loc[:, ['DX_CODE']].values.squeeze()\n",
    "    \n",
    "    # Train the model\n",
    "    hps = high_confidence.TwoStagesPrediction(verbose=False,\n",
    "                                          n_iter=200,\n",
    "                                          shuffle_test_split=0.5,\n",
    "                                            gamma=1,\n",
    "                                          min_gamma=0.95,\n",
    "                                          thresh_ratio=0.8)\n",
    "    hps.fit(X_train, y_train)\n",
    "    scores, dic_results = hps.predict(X_test)\n",
    "    scores_s1_l.append(dic_results['s1_hat'])\n",
    "    scores_s2_l.append(dic_results['s2_hat'])\n",
    "    y_target_l.append(y_test)\n",
    "\n",
    "    current_duration = time.time()-cv_start\n",
    "    took.append(current_duration)\n",
    "    avg_time = np.mean(took)\n",
    "    elapsed_time = np.sum(took)\n",
    "    remaining_time = avg_time * (9-cv_idx)\n",
    "    \n",
    "    print('CV fold {} done. Took {:.2f}s ({:.2f}s), {:.2f}s total, {:.2f}s to go.'.format(cv_idx+1,\n",
    "                                                                              current_duration,\n",
    "                                                                              avg_time,\n",
    "                                                                              elapsed_time,\n",
    "                                                                              remaining_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################\n",
      "Stage 1 (BASE)\n",
      "Class 0 Precision: 58.12 Specificity: 56.04 Recall: 59.04 N: 191\n",
      "Class 1 Precision: 56.98 Specificity: 59.04 Recall: 56.04 N: 179\n",
      "Total Precision: 57.55 Specificity: 57.54 Recall: 57.54 N: 185\n",
      "Stage 2 (HPS)\n",
      "Class 0 Precision: 67.65 Specificity: 93.96 Recall: 12.23 N: 34\n",
      "Class 1 Precision: 83.87 Specificity: 97.34 Recall: 14.29 N: 31\n",
      "Total Precision: 75.76 Specificity: 95.65 Recall: 13.26 N: 32\n",
      "##########################\n"
     ]
    }
   ],
   "source": [
    "y = sample.DX_CODE.values.squeeze()\n",
    "ohe = skl.preprocessing.OneHotEncoder(sparse=False)\n",
    "ohe.fit(y.reshape(-1, 1))\n",
    "labels = ohe.transform(y.reshape(-1, 1))\n",
    "\n",
    "scores_s1_arr = np.vstack(scores_s1_l)\n",
    "scores_s2_arr = np.vstack(scores_s2_l)\n",
    "y_target_arr = np.hstack(y_target_l)\n",
    "\n",
    "########################\n",
    "print('##########################')\n",
    "# S1\n",
    "y_mb = ohe.transform(y_target_arr[:,np.newaxis])\n",
    "pred_y_ = scores_s1_arr\n",
    "\n",
    "print('Stage 1 (BASE)')\n",
    "hps_visu.print_scores(hps_visu.scores(y_mb, pred_y_))\n",
    "\n",
    "\n",
    "# S2\n",
    "y_mb = ohe.transform(y_target_arr[:,np.newaxis])\n",
    "pred_y_ = scores_s2_arr\n",
    "\n",
    "print('Stage 2 (HPS)')\n",
    "hps_visu.print_scores(hps_visu.scores(y_mb, pred_y_)) \n",
    "print('##########################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- all sites, only brain: Precision: 80.00 Specificity: 96.81 Recall: 13.19 N: 30\n",
    "- 'BV', 'AGE_AT_SCAN': Precision: 78.38 Specificity: 95.74 Recall: 15.93 N: 37\n",
    "- ['BV', 'AGE_AT_SCAN']: Precision: 90.91 Specificity: 99.47 Recall:  5.49 N: 11, gamma=1\n",
    "- min_gamma=0.95, thresh_ratio=0.4: Precision: 78.38 Specificity: 95.74 Recall: 15.93 N: 37\n",
    "- gamma=1,min_gamma=0.98,thresh_ratio=0.4: Precision: 100.00 Specificity: 100.00 Recall:  8.79 N: 16\n",
    "- only brain: Precision: 83.33 Specificity: 97.87 Recall: 10.99 N: 24\n",
    "- iter = 1000: Precision: 78.26 Specificity: 97.34 Recall:  9.89 N: 23\n",
    "- thresh: 0.6: Precision: 79.17 Specificity: 97.34 Recall: 10.44 N: 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('/home/surchs/sim_big/PROJECT/abide_hps/train_sbt_0_12.npz')\n",
    "aa = a['train_weights_fc']\n",
    "b = np.load('/home/surchs/sim_big/PROJECT/abide_hps/train_sbt_0_12_onlybrain.npz')\n",
    "bb = b['train_weights_fc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(332, 4, 12)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(332, 4, 12)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numpy.lib.npyio.NpzFile at 0x2ab6b0a95668>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sub = sample.shape[0]\n",
    "\n",
    "\n",
    "print('Need to Run all')\n",
    "# Replicate the subtyping process\n",
    "# Extract the train and test data and regress nuisance factors\n",
    "resid_fc = regress_fc(sample, \n",
    "                           'AGE_AT_SCAN + FD_scrubbed + Site', \n",
    "                           n_vox, n_seed=n_seed, \n",
    "                           seed_p=seed_p, sd_t=sd_t)\n",
    "(resid_ct, mod_ct) = regress_ct(sample, 'AGE_AT_SCAN + Site', ct_p, ct_t)\n",
    "\n",
    "# Make the subtypes from the train data\n",
    "(subtypes_fc, weights_fc) = make_subtype_fc(resid_fc, n_subtypes=n_subtypes)\n",
    "(subtypes_ct, weights_ct) = make_subtype_ct(resid_ct, n_subtypes=n_subtypes)\n",
    "\n",
    "# Build input data\n",
    "fc = np.reshape(weights_fc, (n_sub, n_subtypes*n_seed))\n",
    "\n",
    "features = np.concatenate((weights_ct, fc), 1)\n",
    "\n",
    "\n",
    "# Make sure we use the correct index or else there will be NaNs in the weight columns\n",
    "feat_data = pd.DataFrame(data=features, columns=cols)\n",
    "data = sample.join(feat_data)\n",
    "\n",
    "# Select the features\n",
    "scaler = skl.preprocessing.StandardScaler()\n",
    "x_ = data.loc[:, col_features]\n",
    "# Normalize\n",
    "X = scaler.fit_transform(x_)\n",
    "# Take the numeric diagnosis code, 0 is control, 1 is autism\n",
    "y = data.loc[:, ['DX_CODE']].values.squeeze()\n",
    "\n",
    "# Train the model\n",
    "hps = high_confidence.TwoStagesPrediction(verbose=False,\n",
    "                                      n_iter=1000,\n",
    "                                      shuffle_test_split=0.5,\n",
    "                                        gamma=1,\n",
    "                                      min_gamma=0.95,\n",
    "                                      thresh_ratio=0.3)\n",
    "\n",
    "\n",
    "hps.fit(X, y)\n",
    "res_hitproba = hps.training_hit_probability\n",
    "plt.figure()\n",
    "plt.title('Class 0 hit probability distribution')\n",
    "plt.hist(hps.training_hit_probability[y==0],10);\n",
    "plt.figure()\n",
    "plt.title('Class 1 hit probability distribution')\n",
    "plt.hist(hps.training_hit_probability[y==1],10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps_base_rate = np.sum(hps.training_hit_probability[y==1]>0.9)/len(hps.training_hit_probability[y==1])\n",
    "print('HPS base rate {:.2f}%'.format(hps_base_rate*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the contributing FC features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature weights of the second stage for class 2 (ASD)\n",
    "feature_weights = hps.confidencemodel.clfs[1].coef_\n",
    "non_zero_features = np.where(feature_weights!=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dc = {key:list() for key in ['net', 'sid', 'weight', 'name', 'net_name', 'kind', 'nonzero', 'risk']}\n",
    "for i in range(len(col_features)):\n",
    "    f_name = col_features[i]\n",
    "    if 'fc' in f_name:\n",
    "        # FC\n",
    "        net = col_params[i][0]\n",
    "        sid = col_params[i][1]\n",
    "        net_name = label.loc[label.roi==net+1].name.values[0]\n",
    "        kind = 'FC'\n",
    "    elif 'ct' in f_name:\n",
    "        # CT\n",
    "        net = None\n",
    "        sid = col_params[i][1]\n",
    "        net_name = None\n",
    "        kind = 'CT'\n",
    "    else:\n",
    "        # Behaviour\n",
    "        net = None\n",
    "        sid = None\n",
    "        net_name = None\n",
    "        net_name = None\n",
    "        kind = 'Other'\n",
    "    weight = feature_weights.flatten()[i]\n",
    "    \n",
    "    \n",
    "    f_dc['net'].append(net)\n",
    "    f_dc['sid'].append(sid)\n",
    "    f_dc['weight'].append(weight)\n",
    "    f_dc['name'].append(f_name)\n",
    "    f_dc['net_name'].append(net_name)\n",
    "    f_dc['kind'].append(kind)\n",
    "    f_dc['nonzero'].append(weight!=0)\n",
    "    f_dc['risk'].append(weight>0)\n",
    "feature_data = pd.DataFrame(f_dc)\n",
    "feature_nonzero = feature_data.loc[feature_data.nonzero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sorted = feature_nonzero.sort_values(by=['net', 'sid'])\n",
    "f_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(feature_data.nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sorted = feature_nonzero.sort_values(by=['net', 'sid'])\n",
    "tmp_feat_sort_ind = f_sorted.index.values\n",
    "f_sorted.reset_index(drop=True, inplace=True)\n",
    "\n",
    "net_ind = list()\n",
    "net_border = list()\n",
    "lbl = list()\n",
    "rbl = list()\n",
    "for nid in range(n_seed):\n",
    "    # Find the networks hits\n",
    "    h_ind = f_sorted.loc[f_sorted.net==nid].index.values\n",
    "    if not h_ind[0]-0.5 in net_border:\n",
    "        net_border.append(h_ind[0]-0.5)\n",
    "    net_border.append(h_ind[-1]+0.5)\n",
    "    lbl.append( h_ind[0]-0.5)\n",
    "    rbl.append( h_ind[-1]+0.5)\n",
    "        \n",
    "    if len(h_ind)==2:\n",
    "        net_ind.append(h_ind[0]+0.5)\n",
    "    else:\n",
    "        net_ind.append(h_ind[1])\n",
    "# Add CT and Other\n",
    "net_ind.append(35)\n",
    "net_ind.append(38)\n",
    "c_ind = np.array(net_ind)\n",
    "b_ind = np.array(net_border)\n",
    "lb = np.array(lbl)\n",
    "rb = np.array(rbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sorted.net.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid.parasite_axes import SubplotHost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(f_sorted.iloc[c_ind].net_name.values)\n",
    "names[-2] = 'Cortical Thickness'\n",
    "names[-1] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-order the networks so the effects don't overlap\n",
    "new_net_order = ['AUDITORY_NETWORK_and_POSTERIOR_INSULA',\n",
    "                 'MESOLIMBIC_NETWORK',\n",
    "                 'DEFAULT_MODE_NETWORK_lateral',\n",
    "                 'SOMATOMOTOR_NETWORK',\n",
    "                 'DEFAULT_MODE_NETWORK_posteromedial',\n",
    "                 'DEFAULT_MODE_NETWORK_anteromedial_and_left_ANGULAR_GYRUS',\n",
    "                 'BASAL_GANGLIA_and_THALAMUS',\n",
    "                 'VENTRAL_VISUAL_STREAM_and_DORSAL_VISUAL_STREAM',\n",
    "                 'FRONTO_PARIETAL_NETWORK',\n",
    "                 'VENTRAL_ATTENTION_NETWORK_and_SALIENCE_NETWORK',\n",
    "                 'VISUAL_NETWORK',\n",
    "                 'CEREBELLUM']\n",
    "new_net_names = new_net_order + ['Cortical Thickness', 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_ind_l = list()\n",
    "# Get the FC indices\n",
    "for i in new_net_order:\n",
    "    for j in f_sorted.loc[f_sorted.net_name==i].index.values:\n",
    "        net_ind_l.append(j)\n",
    "# Add the other indices\n",
    "for j in f_sorted.loc[f_sorted.net_name.isnull()].index.values:\n",
    "    net_ind_l.append(j)\n",
    "new_net_ind = np.array(net_ind_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all the non-zero data\n",
    "\n",
    "f = plt.figure(figsize=(3,15), constrained_layout=True)\n",
    "#ax = SubplotHost(f, 111)\n",
    "ax = f.add_subplot(111)\n",
    "for idx, (l, r) in enumerate(zip(lbl, rbl)):\n",
    "    if idx%2==0:\n",
    "        ax.axhspan(l, r, facecolor='lightgrey', alpha=0.2)\n",
    "    else:\n",
    "        ax.axhspan(l, r, facecolor='grey', alpha=0.2)\n",
    "    ax.axhspan(33.5, 36.5, facecolor='lightblue', alpha=0.2)\n",
    "    ax.axhspan(36.5, 39.5, facecolor='darkgrey', alpha=0.2)\n",
    "#f.add_subplot(ax)\n",
    "#offset = 0, -25\n",
    "#ax2 = ax.twiny()\n",
    "#g = sbn.barplot(x='feature', y='weights', data=weights, hue='feature_type', ax=ax)\n",
    "g = sbn.barplot(x='weight', y='name', data=f_sorted.iloc[new_net_ind], hue='kind', ax=ax, ci=None, dodge=False, \n",
    "                palette=sbn.xkcd_palette(['yellow orange', 'cerulean', 'light grey']), orient='h')\n",
    "#g.set(xlabel='Feature Weights of HPS model', ylabel='')\n",
    "sbn.despine(left=True);\n",
    "#ax.set_xticklabels(ax.get_xticklabels(), rotation=90);\n",
    "ax.set_yticks(c_ind)\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "ax.set_yticklabels([])\n",
    "ax.set_yticks([])\n",
    "ax.axvline(0, color='black')\n",
    "#for i in b_ind:\n",
    "#    ax.axvline(i, ymin=0.4, ymax=0.5, color='grey')\n",
    "#new_axisline = ax2.get_grid_helper().new_fixed_axis\n",
    "#ax2.axis[\"bottom\"] = new_axisline(loc=\"bottom\", axes=ax2, offset=offset)\n",
    "ax.legend_.remove()\n",
    "\n",
    "f.savefig(os.path.join(out_p, 'non-zero-features_no_name.png'), dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd_indices = sample.loc[y==1].index.values\n",
    "hps_indices = asd_indices[np.where(hps.training_hit_probability[y==1]>0.95)]\n",
    "nonhps_indices = asd_indices[np.where(hps.training_hit_probability[y==1]<=0.95)]\n",
    "cond_arr = np.zeros(n_sub)\n",
    "cond_arr[hps_indices] = 2\n",
    "cond_arr[nonhps_indices] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the traces\n",
    "features_all = data.loc[:, col_features].values\n",
    "# Sort features to what I use in the rest of the plots\n",
    "features_resort_like_others = features_all[:, tmp_feat_sort_ind]\n",
    "\n",
    "feat_scaler = skl.preprocessing.StandardScaler()\n",
    "features_all_scaled = feat_scaler.fit_transform(features_resort_like_others)\n",
    "n_nonzero_features = features_resort_like_others.shape[1]\n",
    "nonzero_feat_name = f_sorted.name.values\n",
    "# Resort the features again according to the new networks\n",
    "\n",
    "features_all_scaled_resorted = features_all_scaled[:, new_net_ind]\n",
    "feat_names_resorted = list(nonzero_feat_name[new_net_ind])\n",
    "\n",
    "trace_data = {'sub':[rid for rid, row in sample.iterrows() for fid in range(n_nonzero_features)],\n",
    "              'cond':[cond_arr[rid] for rid, row in sample.iterrows() for fid in range(n_nonzero_features)],\n",
    "              'weight':[features_all_scaled_resorted[rid, fid] for rid, row in sample.iterrows() for fid in range(n_nonzero_features)],\n",
    "              'feat':[fid for rid, row in sample.iterrows() for fid in range(n_nonzero_features)]}\n",
    "trace = pd.DataFrame(trace_data)\n",
    "trace['condition'] = trace['cond'].replace({0:'TDC', 1:'iASD', 2:'pASD'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_names = [col_features[i] for i in nonzero_feat_ind[new_net_ind]]\n",
    "\n",
    "f = plt.figure(figsize=(20,10), constrained_layout=True)\n",
    "#ax = SubplotHost(f, 111)\n",
    "ax = f.add_subplot(111)\n",
    "qq = sbn.tsplot(time=\"feat\", value=\"weight\",\n",
    "           unit=\"sub\", condition=\"condition\",data=trace, ci=[10,50,90], ax=ax)\n",
    "ax.set_xticks(np.arange(n_nonzero_features))\n",
    "ax.set_xticklabels(feat_names_resorted, rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all the non-zero data\n",
    "\n",
    "f = plt.figure(figsize=(10,20), constrained_layout=True)\n",
    "#ax = SubplotHost(f, 111)\n",
    "ax = f.add_subplot(111)\n",
    "for idx, (l, r) in enumerate(zip(lbl, rbl)):\n",
    "    if idx%2==0:\n",
    "        ax.axhspan(l, r, facecolor='lightgrey', alpha=0.2)\n",
    "    else:\n",
    "        ax.axhspan(l, r, facecolor='grey', alpha=0.2)\n",
    "    ax.axhspan(33.5, 36.5, facecolor='lightblue', alpha=0.2)\n",
    "    ax.axhspan(36.5, 39.5, facecolor='darkgrey', alpha=0.2)\n",
    "#f.add_subplot(ax)\n",
    "#offset = 0, -25\n",
    "#ax2 = ax.twiny()\n",
    "#g = sbn.barplot(x='feature', y='weights', data=weights, hue='feature_type', ax=ax)\n",
    "g = sbn.barplot(x='weight', y='name', data=f_sorted.iloc[new_net_ind], hue='kind', ax=ax, ci=None, dodge=False, \n",
    "                palette=sbn.xkcd_palette(['yellow orange', 'cerulean', 'light grey']), orient='h')\n",
    "g.set(xlabel='Feature Weights of HPS model', ylabel='')\n",
    "sbn.despine();\n",
    "#ax.set_xticklabels(ax.get_xticklabels(), rotation=90);\n",
    "ax.set_yticks(c_ind)\n",
    "ax.set_yticklabels(new_net_names, rotation=0)\n",
    "ax.axvline(0, color='black')\n",
    "#for i in b_ind:\n",
    "#    ax.axvline(i, ymin=0.4, ymax=0.5, color='grey')\n",
    "#new_axisline = ax2.get_grid_helper().new_fixed_axis\n",
    "#ax2.axis[\"bottom\"] = new_axisline(loc=\"bottom\", axes=ax2, offset=offset)\n",
    "ax.legend_.remove()\n",
    "\n",
    "f.savefig(os.path.join(out_p, 'non-zero-features.png'), dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all the non-zero data\n",
    "f = plt.figure(figsize=(15.5,4), constrained_layout=True)\n",
    "#ax = SubplotHost(f, 111)\n",
    "ax = f.add_subplot(111)\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['bottom'].set_visible(False)\n",
    "\n",
    "for idx, (l, r) in enumerate(zip(lbl, rbl)):\n",
    "    if idx%2==0:\n",
    "        ax.axvspan(l, r, facecolor='lightgrey', alpha=0.2)\n",
    "    else:\n",
    "        ax.axvspan(l, r, facecolor='grey', alpha=0.2)\n",
    "ax.axvspan(33.5, 36.5, facecolor='lightgrey', alpha=0.2)\n",
    "ax.axvspan(36.5, 39.5, facecolor='grey', alpha=0.2)\n",
    "\n",
    "#g = sbn.barplot(x='feature', y='weights', data=weights, hue='feature_type', ax=ax)\n",
    "g = sbn.barplot(x='name', y='weight', data=f_sorted.iloc[new_net_ind], hue='kind', ax=ax, ci=None, dodge=False, \n",
    "                edgecolor=\"black\", facecolor='white')\n",
    "                #palette=sbn.xkcd_palette(['yellow orange', 'cerulean', 'light grey']), orient='v') #\n",
    "#g.set(xlabel='Feature Weights of HPS model', ylabel='')\n",
    "\n",
    "qq = sbn.tsplot(time=\"feat\", value=\"weight\",\n",
    "           unit=\"sub\", condition=\"condition\",data=trace, ci=[95], ax=ax2,\n",
    "               color=sbn.xkcd_palette(['green', 'cerulean', 'gold']), alpha=0.5)\n",
    "#ax.set_xticks(np.arange(n_nonzero_features))\n",
    "#ax.set_xticklabels(feat_names_resorted, rotation=90);\n",
    "#sbn.despine(bottom=True);\n",
    "\n",
    "#ax.set_yticks(c_ind)\n",
    "#ax.set_yticklabels(names, rotation=0)\n",
    "ax.axhline(0, color='black')\n",
    "#for i in b_ind:\n",
    "#    ax.axvline(i, ymin=0.4, ymax=0.5, color='grey')\n",
    "#new_axisline = ax2.get_grid_helper().new_fixed_axis\n",
    "#ax2.axis[\"bottom\"] = new_axisline(loc=\"bottom\", axes=ax2, offset=offset)\n",
    "ax.legend_.remove()\n",
    "ax.set_xticks(np.arange(n_nonzero_features)[-3:]);\n",
    "ax.set_xticklabels(['Brain volume', 'Age', 'Head motion'], rotation=90)\n",
    "ax.set_xlabel('');\n",
    "ax.set_ylabel('Feature weight');\n",
    "ax2.set_ylabel('Feature loading');\n",
    "ax2.legend(loc='center left', bbox_to_anchor=(0.93, 0.85))\n",
    "ax.set_xlim([-0.5, 39.5])\n",
    "\n",
    "\n",
    "f.savefig(os.path.join(out_p, 'non-zero-features_horizontal.png'), dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_data = feature_data.loc[feature_data.kind=='FC']\n",
    "fc_data.sort_values(by='weight', inplace=True)\n",
    "fc_data[['net', 'sid']] = fc_data[['net', 'sid']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many networks are involved in the bottom 5 features\n",
    "protective_fc = fc_data.iloc[:5]['net'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_data.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many networks are involved in the top 5 features\n",
    "risk_fc = fc_data.iloc[::-1][:5]['net'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_data.iloc[::-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(risk_fc).union(protective_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(risk_fc).union(protective_fc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show me the top 5 features\n",
    "for rid, (tmp, row) in enumerate(fc_data.iloc[::-1][:5].iterrows()):\n",
    "    f = plt.figure(figsize=(8,4), constrained_layout=True)\n",
    "    ax1 = f.add_subplot(111) \n",
    "    #ax2 = f.add_subplot(122)\n",
    "    # Show me the subtype\n",
    "    n_name = label.loc[label.roi==row['net']+1].name.values[0]\n",
    "    vol = np.zeros(mask.shape)\n",
    "    vol[mask] = subtypes_fc[row['sid'], :, row['net']]\n",
    "    img = nib.Nifti1Image(vol, affine=mask_i.affine)\n",
    "    \n",
    "    a_vol = np.zeros(mask.shape)\n",
    "    a_vol[atlas==row['net']+1] = 1\n",
    "    a_img = nib.Nifti1Image(a_vol, affine=mask_i.affine)\n",
    "    \n",
    "    display = nlp.plot_stat_map(img, cut_coords=(0,0,0), axes=ax1, colorbar=False, draw_cross=False, cmap=plt.cm.RdBu_r)\n",
    "    display.add_contours(a_img, levels=[.5], colors='black') \n",
    "    \n",
    "    a_vol = np.zeros(mask.shape)\n",
    "    a_vol[atlas==row['net']+1] = 1\n",
    "    a_img = nib.Nifti1Image(a_vol, affine=mask_i.affine)\n",
    "    f.savefig(os.path.join(out_p, 'fc_sbt_risk_{}_{}_{}.png'.format(rid, row['sid']+1, n_name)), dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show me the top 5 features\n",
    "for rid, (tmp, row) in enumerate(fc_data.iloc[:5].iterrows()):\n",
    "    f = plt.figure(figsize=(8,4), constrained_layout=True)\n",
    "    ax1 = f.add_subplot(111) \n",
    "    #ax2 = f.add_subplot(122)\n",
    "    # Show me the subtype\n",
    "    n_name = label.loc[label.roi==row['net']+1].label.values[0]\n",
    "    vol = np.zeros(mask.shape)\n",
    "    vol[mask] = subtypes_fc[row['sid'], :, row['net']]\n",
    "    img = nib.Nifti1Image(vol, affine=mask_i.affine)\n",
    "    \n",
    "    a_vol = np.zeros(mask.shape)\n",
    "    a_vol[atlas==row['net']+1] = 1\n",
    "    a_img = nib.Nifti1Image(a_vol, affine=mask_i.affine)\n",
    "    \n",
    "    display = nlp.plot_stat_map(img, cut_coords=(0,0,0), axes=ax1, colorbar=False, draw_cross=False, cmap=plt.cm.RdBu_r)\n",
    "    display.add_contours(a_img, levels=[.5], colors='black') \n",
    "    \n",
    "    a_vol = np.zeros(mask.shape)\n",
    "    a_vol[atlas==row['net']+1] = 1\n",
    "    a_img = nib.Nifti1Image(a_vol, affine=mask_i.affine)\n",
    "    f.savefig(os.path.join(out_p, 'fc_sbt_protective_{}_{}_{}.png'.format(rid, row['sid']+1, n_name)), dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the average maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_fc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(avg_fc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nid in range(n_seed):\n",
    "    f = plt.figure(figsize=(8,4))\n",
    "    ax1 = f.add_subplot(111) \n",
    "    #ax2 = f.add_subplot(122)\n",
    "    # Show me the subtype\n",
    "    n_name = label.loc[label.roi==nid+1].label.values[0]\n",
    "    vol = np.zeros(mask.shape)\n",
    "    vol[mask] = avg_fc[:, nid]\n",
    "    img = nib.Nifti1Image(vol, affine=mask_i.affine)\n",
    "    \n",
    "    a_vol = np.zeros(mask.shape)\n",
    "    a_vol[atlas==nid+1] = 1\n",
    "    a_img = nib.Nifti1Image(a_vol, affine=mask_i.affine)\n",
    "    \n",
    "    display = nlp.plot_anat(cut_coords=(0,0,0), axes=ax1, draw_cross=False)\n",
    "    display.add_overlay(img, cmap=plt.cm.viridis, colorbar=False, vmin=0)\n",
    "    display.add_contours(a_img, levels=[.5], colors='black')\n",
    "    f.savefig(os.path.join(out_p, 'fc_avg_{}.png'.format(n_name)), dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the CT subtypes\n",
    "for i in range(n_subtypes):\n",
    "    ct_l = subtypes_ct[i, :40962]\n",
    "    ct_r = subtypes_ct[i, :40962]\n",
    "    np.savetxt(os.path.join(out_p, 'ct_{}_left.txt'.format(i)),ct_l)\n",
    "    np.savetxt(os.path.join(out_p, 'ct_{}_right.txt'.format(i)),ct_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get best slice for network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_data.loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which are the features that are not zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the average positive feature (first 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_idx[:10]:\n",
    "    print(col_params[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_feat = np.zeros(subtypes_fc.shape[1])\n",
    "q = 0\n",
    "for i in sort_idx[-3:]:\n",
    "    tmp = col_params[i]\n",
    "    if feature_weights[:, i]<0:\n",
    "        continue\n",
    "    if tmp[0] is None:\n",
    "        continue\n",
    "    if tmp[0]<0:\n",
    "        continue\n",
    "    pos_feat += subtypes_fc[tmp[1], :, tmp[0]]\n",
    "    q+=1\n",
    "pos_feat = pos_feat/q\n",
    "\n",
    "\n",
    "pos_vol = np.zeros(mask.shape)\n",
    "pos_vol[mask] = pos_feat\n",
    "pos_img = nib.Nifti1Image(pos_vol, affine=mask_i.affine)\n",
    "nlp.plot_stat_map(pos_img, cut_coords=(0,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_feat = np.zeros(subtypes_fc.shape[1])\n",
    "q = 0\n",
    "for i in sort_idx[:10]:\n",
    "    tmp = col_params[i]\n",
    "    if feature_weights[:, i]>0:\n",
    "        continue\n",
    "    if tmp[0] is None:\n",
    "        continue\n",
    "    if tmp[0]<0:\n",
    "        continue\n",
    "    neg_feat += subtypes_fc[tmp[1], :, tmp[0]]\n",
    "    q+=1\n",
    "neg_feat = neg_feat/q\n",
    "\n",
    "\n",
    "neg_vol = np.zeros(mask.shape)\n",
    "neg_vol[mask] = neg_feat\n",
    "neg_img = nib.Nifti1Image(neg_vol, affine=mask_i.affine)\n",
    "nlp.plot_stat_map(neg_img, cut_coords=(0,0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt a validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sample.DX_CODE.values.squeeze()\n",
    "ohe = skl.preprocessing.OneHotEncoder(sparse=False)\n",
    "ohe.fit(y.reshape(-1, 1))\n",
    "labels = ohe.transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "validation_p = os.path.join(root_p, 'PROJECT/abide_hps/pheno', 'validation_abide1.csv')\n",
    "validation = pd.read_csv(validation_p)\n",
    "validation['DX_CODE'] = validation['DX_GROUP'].replace({'Autism':1, 'Control':0})\n",
    "n_sub_valid = validation.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_resid_fc = regress_fc(validation, \n",
    "                            'AGE_AT_SCAN + FD_scrubbed + Site', \n",
    "                            n_vox, n_seed=n_seed, \n",
    "                            seed_p=seed_p, sd_t=sd_t)\n",
    "(valid_resid_ct, valid_mod_ct) = regress_ct(validation, 'AGE_AT_SCAN + Site', ct_p, ct_t)\n",
    "valid_weights_fc = make_weights_fc(subtypes_fc[0], valid_resid_fc)\n",
    "valid_weights_ct = make_weights_ct(subtypes_ct[0], valid_resid_ct)\n",
    "\n",
    "valid_fc = np.reshape(valid_weights_fc, (n_sub_valid, n_subtypes*n_seed))\n",
    "valid_w = np.concatenate((valid_weights_ct, valid_fc), 1)\n",
    "\n",
    "w_data_valid = pd.DataFrame(data=valid_w, columns=cols)\n",
    "data_valid = validation.join(w_data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = data_valid.loc[:, col_features]\n",
    "X_valid = scaler.transform(x_valid)\n",
    "y_valid = data_valid.loc[:, ['DX_CODE']].values.squeeze()\n",
    "\n",
    "scores_valid, dic_results_valid = hps.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_s1_valid = dic_results_valid['s1_hat']\n",
    "scores_s2_valid = dic_results_valid['s2_hat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "print('##########################')\n",
    "# S1\n",
    "y_mb_valid = ohe.transform(y_valid[:,np.newaxis])\n",
    "pred_y_valid = scores_s1_valid\n",
    "\n",
    "print('Stage 1 (BASE)')\n",
    "hps_visu.print_scores(hps_visu.scores(y_mb_valid, pred_y_valid))\n",
    "\n",
    "\n",
    "# S2\n",
    "y_mb = ohe.transform(y_valid[:,np.newaxis])\n",
    "pred_y_valid = scores_s2_valid\n",
    "\n",
    "print('Stage 2 (HPS)')\n",
    "hps_visu.print_scores(hps_visu.scores(y_mb_valid, pred_y_valid)) \n",
    "print('##########################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that isn't so amazing now, is it? But it's also not worlds apart in terms of specificity. I mean the precision suffers greatly and overall the model doesn't appear that nice anymore. But it's not so horrible now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = skl.metrics.confusion_matrix(y_mb_valid[:, 1], scores_s2_valid[: , 1]).ravel().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tell me something about these guys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPS_ind_valid = pred_y_valid[:, 1]==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give me three classes here\n",
    "asd_hps_valid = [True if HPS_ind_valid[rid]==1 and row['DX_GROUP']=='Autism' else False for rid, row in validation.iterrows()]\n",
    "asd_ns_valid = [True if HPS_ind_valid[rid]==0 and row['DX_GROUP']=='Autism' else False for rid, row in validation.iterrows()]\n",
    "tdc_hps_valid = [True if HPS_ind_valid[rid]==1 and row['DX_GROUP']=='Control' else False for rid, row in validation.iterrows()]\n",
    "tdc_ns_valid = [True if HPS_ind_valid[rid]==0 and row['DX_GROUP']=='Control' else False for rid, row in validation.iterrows()]\n",
    "group = list()\n",
    "for rid, row in validation.iterrows():\n",
    "    if asd_hps_valid[rid]:\n",
    "        group.append('ASD_HPS')\n",
    "    elif asd_ns_valid[rid]:\n",
    "        group.append('ASD_NS')\n",
    "    elif tdc_hps_valid[rid]:\n",
    "        group.append('TDC_HPS')\n",
    "    else:\n",
    "        group.append('TDC_NS')\n",
    "results_validation = validation.copy()\n",
    "results_validation['Group'] = group\n",
    "# Remove missing values\n",
    "results_validation.replace({col:{-9999:None} for col in results_validation.columns}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.barplot(x='Group', y='AGE_AT_SCAN', data=results_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.barplot(x='Group', y='BV', data=results_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.barplot(x='Group', y='FD_scrubbed', data=results_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.barplot(x='Group', y='FIQ', data=results_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = scores_s2_arr\n",
    "HPS_ind = y_pred[:, 1]==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give me three classes here\n",
    "asd_hps = [True if HPS_ind[rid]==1 and row['DX_GROUP']=='Autism' else False for rid, row in sample.iterrows()]\n",
    "asd_ns = [True if HPS_ind[rid]==0 and row['DX_GROUP']=='Autism' else False for rid, row in sample.iterrows()]\n",
    "tdc = [True if row['DX_GROUP']=='Control' else False for rid, row in sample.iterrows()]\n",
    "group = list()\n",
    "for rid, row in sample.iterrows():\n",
    "    if asd_hps[rid]:\n",
    "        group.append('ASD_HPS')\n",
    "    elif asd_ns[rid]:\n",
    "        group.append('ASD_NS')\n",
    "    else:\n",
    "        group.append('TDC')\n",
    "results = sample.copy()\n",
    "results['Group'] = group\n",
    "# Remove missing values\n",
    "results.replace({col:{-9999:None} for col in results.columns}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.barplot(x='Group', y='AGE_AT_SCAN', data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = ols('AGE_AT_SCAN ~ Group',\n",
    "               data=results).fit()\n",
    "table = sm.stats.anova_lm(lm, typ=2)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.barplot(x='Group', y='BV', data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = ols('BV ~ Group',\n",
    "               data=results).fit()\n",
    "table = sm.stats.anova_lm(lm, typ=2)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.barplot(x='Group', y='FD_scrubbed', data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = ols('FD_scrubbed ~ Group',\n",
    "               data=results).fit()\n",
    "table = sm.stats.anova_lm(lm, typ=2)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.barplot(x='Group', y='FIQ', data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.barplot(x='Group', y='VIQ', data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.barplot(x='Group', y='PIQ', data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.barplot(x='Group', y='SRS_RAW_TOTAL', data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.barplot(x='Group', y='Gotham_Severity', data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.barplot(x='Group', y='HANDEDNESS_SCORES', data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(18, 6))\n",
    "ax1 = f.add_subplot(131)\n",
    "ax2 = f.add_subplot(132)\n",
    "ax3 = f.add_subplot(133)\n",
    "for rid, g in enumerate(results.groupby('Group')):\n",
    "    ax = f.add_subplot(1,3,rid+1)\n",
    "    g[1]['DSM_IV_TR'].value_counts().plot.pie(ax=ax)\n",
    "    ax.set_title(g[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36 (hbm)",
   "language": "python",
   "name": "hbm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
