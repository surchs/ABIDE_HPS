{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search HPS model\n",
    "- get the inputs correct\n",
    "- run through all steps\n",
    "- get an accuracy estimate\n",
    "- now with subtype weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "sys.path.append('/home/surchs/git/HPS')\n",
    "from hps.predic import high_confidence\n",
    "from hps.visu import hps_visu\n",
    "sys.path.append('/home/surchs/git/HPS/examples/')\n",
    "import visu_demo\n",
    "import scipy as sp\n",
    "import patsy as pat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import sklearn as skl\n",
    "import scipy.io as sio\n",
    "import seaborn as sbn\n",
    "from scipy import cluster as scl\n",
    "from scipy import stats as spt\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model as sln\n",
    "from sklearn import preprocessing as skp\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seed = 7\n",
    "n_subtypes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "root_p = '/home/surchs/sim_big/PROJECT/abide_hps/'\n",
    "# Pheno\n",
    "sample_p = os.path.join(root_p, 'pheno', 'psm_abide1.csv')\n",
    "# Data\n",
    "ct_p = os.path.join(root_p, 'ct')\n",
    "seed_p = os.path.join(root_p, 'seed', 'MIST_{}'.format(n_seed))\n",
    "mask_p = os.path.join(root_p, 'mask', 'MIST_mask.nii.gz')\n",
    "label_p = os.path.join(root_p, 'mask', 'roi_label_scale_20_overlap.csv')\n",
    "# File templates\n",
    "ct_t = '{}+{:07}_{}+{}_native_rms_rsl_tlaplace_30mm_{}.txt'\n",
    "sd_t = 'sub_{{}}_mist_{}.npy'.format(n_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "sample = pd.read_csv(sample_p)\n",
    "sample['DX_CODE'] = sample['DX_GROUP'].replace({'Autism':1, 'Control':0})\n",
    "label = pd.read_csv(label_p, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_i = nib.load(mask_p)\n",
    "mask = mask_i.get_data().astype(bool)\n",
    "n_vox = np.sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the CV model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2_coeff(A,B):\n",
    "    # Rowwise mean of input arrays & subtract from input arrays themeselves\n",
    "    A_mA = A - A.mean(1)[:,None]\n",
    "    B_mB = B - B.mean(1)[:,None]\n",
    "\n",
    "    # Sum of squares across rows\n",
    "    ssA = (A_mA**2).sum(1);\n",
    "    ssB = (B_mB**2).sum(1);\n",
    "\n",
    "    # Finally get corr coeff\n",
    "    return np.dot(A_mA,B_mB.T)/np.sqrt(np.dot(ssA[:,None],ssB[None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtype(stack, n_subtypes):\n",
    "    # Normalize and then get the distance\n",
    "    norm = skp.scale(stack, axis=1)\n",
    "    # Get the lower triangle of the distance metric\n",
    "    dist = sp.spatial.distance.pdist(norm)\n",
    "    # Build the cluster\n",
    "    link = scl.hierarchy.linkage(dist, method='ward')\n",
    "    order = scl.hierarchy.dendrogram(link, no_plot=True)['leaves']\n",
    "    part = scl.hierarchy.fcluster(link, n_subtypes, criterion='maxclust')\n",
    "    return order, part, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress_fc(sample, formula, n_vox, n_seed, seed_p, sd_t):\n",
    "    n_sub = sample.shape[0]\n",
    "    resid_seed = np.zeros((n_sub, n_vox, n_seed))\n",
    "    dmat_seed = pat.dmatrix(formula, data=sample)\n",
    "    for sid in range(n_seed):\n",
    "        # Build the regression model for the seed maps\n",
    "        mod = sln.LinearRegression(fit_intercept=True, normalize=True, n_jobs=-1)\n",
    "        sub_seed = np.zeros((n_sub, n_vox))\n",
    "        # Line index doesn't necessarily match continuous index\n",
    "        for rid, (rid_abs, row) in enumerate(sample.iterrows()):\n",
    "            p = os.path.join(seed_p, sd_t.format(row['SUB_ID']))\n",
    "            d = np.load(p)\n",
    "            sub_seed[rid, :] = d[sid, ...]\n",
    "        res = mod.fit(dmat_seed, sub_seed)\n",
    "        resid = sub_seed - res.predict(dmat_seed)\n",
    "        resid_seed[..., sid] = resid\n",
    "    \n",
    "    return resid_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress_ct(sample, formula, ct_p, ct_t):\n",
    "    n_sub = sample.shape[0]\n",
    "    # Generate the CT residuals\n",
    "    for rid, (rid_abs, row) in enumerate(sample.iterrows()):\n",
    "        p_right = os.path.join(ct_p, ct_t.format(row['Site'], row['Subject'], row['Session'], row['Run'], 'right'))\n",
    "        p_left = os.path.join(ct_p, ct_t.format(row['Site'], row['Subject'], row['Session'], row['Run'], 'left'))\n",
    "        ct_l = pd.read_csv(p_left, header=None)[0].values\n",
    "        ct_r = pd.read_csv(p_right, header=None)[0].values\n",
    "        # Combine left and right\n",
    "        ct = np.concatenate((ct_l, ct_r))\n",
    "        if rid==0:\n",
    "            n_vert = len(ct)\n",
    "            sub_ct = np.zeros((n_sub, n_vert))\n",
    "        sub_ct[rid, :] = ct\n",
    "    dmat_ct = pat.dmatrix(formula, data=sample)\n",
    "    mod = sln.LinearRegression(fit_intercept=True, normalize=True, n_jobs=-1)\n",
    "    res = mod.fit(dmat_ct, sub_ct)\n",
    "    resid_ct = sub_ct - res.predict(dmat_ct)\n",
    "    \n",
    "    return resid_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subtype_fc(resid, n_subtypes):\n",
    "    n_sub, n_vox, n_seed = resid.shape\n",
    "    # Run the FC subtypes\n",
    "    weights_fc = np.zeros((n_sub, n_subtypes, n_seed))\n",
    "    subtypes_fc = np.zeros((n_subtypes,) + resid.shape[1:])\n",
    "    parts_fc = np.zeros((n_sub, n_seed))\n",
    "    orders_fc = np.zeros((n_sub, n_seed))\n",
    "    dists_fc = np.zeros((n_sub, n_sub, n_seed))\n",
    "\n",
    "    for sid in range(n_seed):\n",
    "        order_fc, part_fc, dist_fc = subtype(resid[..., sid], n_subtypes)\n",
    "        dists_fc[..., sid] = sp.spatial.distance.squareform(dist_fc)\n",
    "        parts_fc[:, sid] = part_fc\n",
    "        orders_fc[:, sid] = order_fc\n",
    "        # Make the subtypes\n",
    "        subtypes_fc_tmp = np.array([np.mean(resid[part_fc==i, :, sid], 0) \n",
    "                                    for i in range(1,n_subtypes+1)])\n",
    "        subtypes_fc[..., sid] = subtypes_fc_tmp\n",
    "        # Compute the weights\n",
    "        weights_fc[..., sid] = corr2_coeff(resid[..., sid], subtypes_fc_tmp)\n",
    "    return subtypes_fc, weights_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subtype_ct(resid, n_subtypes):\n",
    "    order_ct, part_ct, dist_ct = subtype(resid, n_subtypes)\n",
    "    # Make the subtypes\n",
    "    subtypes_ct = np.array([np.mean(resid[part_ct==i, :], 0) \n",
    "                            for i in range(1,n_subtypes+1)])\n",
    "    # Compute the weights\n",
    "    weights_ct = corr2_coeff(resid, subtypes_ct)\n",
    "    return (subtypes_ct, weights_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_fc(subtypes, resid):\n",
    "    n_sub, n_vox, n_seed = resid.shape\n",
    "    n_subtypes = subtypes.shape[0]\n",
    "    weights_fc = np.zeros((n_sub, n_subtypes, n_seed))\n",
    "    for sid in range(n_seed):\n",
    "    # Compute the weights\n",
    "        weights_fc[..., sid] = corr2_coeff(resid[..., sid], subtypes[..., sid])\n",
    "    return weights_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_ct(subtypes, resid):\n",
    "    weights_ct = corr2_coeff(resid, subtypes)\n",
    "    return weights_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full range of subject indices and clinical labels\n",
    "sub_indices = sample.index.values\n",
    "labels = sample['DX_CODE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_cols = ['fc_n{}_s{}'.format(nid+1, sid+1) \n",
    "           for sid in range(n_subtypes) \n",
    "           for nid in range(n_seed)]\n",
    "ct_cols = ['ct_s{}'.format(sid+1) \n",
    "           for sid in range(n_subtypes)]\n",
    "cols = ct_cols + fc_cols\n",
    "col_features = ['BV', 'AGE_AT_SCAN', 'FD_scrubbed', ] + cols\n",
    "#col_features = ['BV', 'FD_scrubbed', ] + cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 1 done. Took 167.32s (167.32s), 167.32s total, 1505.84s to go.\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 2 done. Took 163.13s (165.22s), 330.45s total, 1321.79s to go.\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 3 done. Took 174.95s (168.47s), 505.40s total, 1179.27s to go.\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 4 done. Took 119.45s (156.21s), 624.85s total, 937.27s to go.\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 5 done. Took 168.77s (158.72s), 793.62s total, 793.62s to go.\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 6 done. Took 149.45s (157.18s), 943.08s total, 628.72s to go.\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 7 done. Took 140.95s (154.86s), 1084.03s total, 464.58s to go.\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 8 done. Took 171.23s (156.91s), 1255.26s total, 313.82s to go.\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 9 done. Took 126.80s (153.56s), 1382.06s total, 153.56s to go.\n",
      "Stage 1\n",
      "Stage 2\n",
      "CV fold 10 done. Took 162.27s (154.43s), 1544.32s total, 0.00s to go.\n"
     ]
    }
   ],
   "source": [
    "scores_s1_l = list()\n",
    "scores_s2_l = list()\n",
    "y_target_l = list()\n",
    "\n",
    "start = time.time()\n",
    "took = []\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for cv_idx, (train_index, test_index) in enumerate(skf.split(sub_indices, labels)):\n",
    "    cv_start = time.time()\n",
    "    \n",
    "    # Get the train, and test sample\n",
    "    train_sample = sample.loc[train_index]\n",
    "    test_sample = sample.loc[test_index]\n",
    "    n_sub_train = train_sample.shape[0]\n",
    "    n_sub_test = test_sample.shape[0]\n",
    "    \n",
    "    # Replicate the subtyping process\n",
    "    # Extract the train and test data and regress nuisance factors\n",
    "    train_resid_fc = regress_fc(train_sample, \n",
    "                               'AGE_AT_SCAN + FD_scrubbed + Site', \n",
    "                               n_vox, n_seed=n_seed, \n",
    "                               seed_p=seed_p, sd_t=sd_t)\n",
    "    test_resid_fc = regress_fc(test_sample, \n",
    "                              'AGE_AT_SCAN + FD_scrubbed + Site', \n",
    "                              n_vox, n_seed=n_seed, \n",
    "                              seed_p=seed_p, sd_t=sd_t)\n",
    "    train_resid_ct = regress_ct(train_sample, 'AGE_AT_SCAN + Site', ct_p, ct_t)\n",
    "    test_resid_ct = regress_ct(test_sample, 'AGE_AT_SCAN + Site', ct_p, ct_t)\n",
    "    # Make the subtypes from the train data\n",
    "    (subtypes_fc, train_weights_fc) = make_subtype_fc(train_resid_fc, n_subtypes=n_subtypes)\n",
    "    (subtypes_ct, train_weights_ct) = make_subtype_ct(train_resid_ct, n_subtypes=n_subtypes)\n",
    "    # Get the test weights\n",
    "    test_weights_fc = make_weights_fc(subtypes_fc, test_resid_fc)\n",
    "    test_weights_ct = make_weights_ct(subtypes_ct, test_resid_ct)\n",
    "    \n",
    "    # Build input data\n",
    "    train_fc = np.reshape(train_weights_fc, (n_sub_train, n_subtypes*n_seed))\n",
    "    test_fc = np.reshape(test_weights_fc, (n_sub_test, n_subtypes*n_seed))\n",
    "    train_w = np.concatenate((train_weights_ct, train_fc), 1)\n",
    "    test_w = np.concatenate((test_weights_ct, test_fc), 1)\n",
    "    \n",
    "    # Make sure we use the correct index or else there will be NaNs in the weight columns\n",
    "    w_data_train = pd.DataFrame(data=train_w, columns=cols, index=train_index)\n",
    "    data_train = train_sample.join(w_data_train)\n",
    "    w_data_test = pd.DataFrame(data=test_w, columns=cols, index=test_index)\n",
    "    data_test = test_sample.join(w_data_test)\n",
    "    \n",
    "    # Select the features\n",
    "    scaler = skl.preprocessing.StandardScaler()\n",
    "    x_train = data_train.loc[:, col_features]\n",
    "    # Normalize\n",
    "    X_train = scaler.fit_transform(x_train)\n",
    "    # Take the numeric diagnosis code, 0 is control, 1 is autism\n",
    "    y_train = data_train.loc[:, ['DX_CODE']].values.squeeze()\n",
    "\n",
    "    # Same for the test data\n",
    "    x_test = data_test.loc[:, col_features]\n",
    "    # Normalize, but use the fitted scalar of the training data\n",
    "    X_test = scaler.transform(x_test)\n",
    "    y_test = data_test.loc[:, ['DX_CODE']].values.squeeze()\n",
    "    \n",
    "    # Train the model\n",
    "    hps = high_confidence.TwoStagesPrediction(verbose=False,\n",
    "                                          n_iter=1000,\n",
    "                                          shuffle_test_split=0.5,\n",
    "                                            gamma=1,\n",
    "                                          min_gamma=0.95,\n",
    "                                          thresh_ratio=0.2)\n",
    "    hps.fit(X_train, y_train)\n",
    "    scores, dic_results = hps.predict(X_test)\n",
    "    scores_s1_l.append(dic_results['s1_hat'])\n",
    "    scores_s2_l.append(dic_results['s2_hat'])\n",
    "    y_target_l.append(y_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    current_duration = time.time()-cv_start\n",
    "    took.append(current_duration)\n",
    "    avg_time = np.mean(took)\n",
    "    elapsed_time = np.sum(took)\n",
    "    remaining_time = avg_time * (9-cv_idx)\n",
    "    \n",
    "    print('CV fold {} done. Took {:.2f}s ({:.2f}s), {:.2f}s total, {:.2f}s to go.'.format(cv_idx+1,\n",
    "                                                                              current_duration,\n",
    "                                                                              avg_time,\n",
    "                                                                              elapsed_time,\n",
    "                                                                              remaining_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sample.DX_CODE.values.squeeze()\n",
    "ohe = skl.preprocessing.OneHotEncoder(sparse=False)\n",
    "ohe.fit(y.reshape(-1, 1))\n",
    "labels = ohe.transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################\n",
      "Stage 1 (BASE)\n",
      "Class 0 Precision: 58.06 Specificity: 57.14 Recall: 57.45 N: 186\n",
      "Class 1 Precision: 56.52 Specificity: 57.45 Recall: 57.14 N: 184\n",
      "Total Precision: 57.29 Specificity: 57.29 Recall: 57.29 N: 185\n",
      "Stage 2 (HPS)\n",
      "Class 0 Precision: 69.05 Specificity: 92.86 Recall: 15.43 N: 42\n",
      "Class 1 Precision: 62.07 Specificity: 94.15 Recall:  9.89 N: 29\n",
      "Total Precision: 65.56 Specificity: 93.50 Recall: 12.66 N: 35\n",
      "##########################\n"
     ]
    }
   ],
   "source": [
    "scores_s1_arr = np.vstack(scores_s1_l)\n",
    "scores_s2_arr = np.vstack(scores_s2_l)\n",
    "y_target_arr = np.hstack(y_target_l)\n",
    "\n",
    "########################\n",
    "print('##########################')\n",
    "# S1\n",
    "y_mb = ohe.transform(y_target_arr[:,np.newaxis])\n",
    "pred_y_ = scores_s1_arr\n",
    "\n",
    "print('Stage 1 (BASE)')\n",
    "hps_visu.print_scores(hps_visu.scores(y_mb, pred_y_))\n",
    "\n",
    "\n",
    "# S2\n",
    "y_mb = ohe.transform(y_target_arr[:,np.newaxis])\n",
    "pred_y_ = scores_s2_arr\n",
    "\n",
    "print('Stage 2 (HPS)')\n",
    "hps_visu.print_scores(hps_visu.scores(y_mb, pred_y_)) \n",
    "print('##########################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. min_gamma = 0.9, gamma=0.98, min_thresh=0.2, n_iter=1000, split_ratio=0.5:\n",
    "    - prec: 85.71%\n",
    "    - spec: 97.87%\n",
    "    - sens: 13.19%\n",
    "2. min_gamma = 0.85, gamma=1, min_thresh=0.25, n_iter=1000, split_ratio=0.5:\n",
    "    - prec: 75.68%\n",
    "    - spec: 95.21%\n",
    "    - sens: 15.38%\n",
    "3. min_gamma = 0.99, gamma=1, min_thresh=0.25, n_iter=1000, split_ratio=0.5:\n",
    "    - prec: something around 85\n",
    "    - spec: something around 99\n",
    "    - sens: something around 1% (I got 2 labeled).\n",
    "4. min_gamma = 0.96, gamma=1, min_thresh=0.25, n_iter=1000, split_ratio=0.5:\n",
    "    - prec: 66.67\n",
    "    - spec: 97.87\n",
    "    - sens: 4.40 (12)\n",
    "5. min_gamma = 0.9, gamma=1, min_thresh=0.3, n_iter=1000, split_ratio=0.5:\n",
    "    - prec: 85.71\n",
    "    - spec: 97.81\n",
    "    - sens: 13.19\n",
    "6. min_gamma = 0.7, gamma=1, min_thresh=0.3, n_iter=1000, split_ratio=0.5:\n",
    "    - prec: 70.27\n",
    "    - spec: 88.30\n",
    "    - sens: 28.12\n",
    "7. N_sbt=3 (so far 5). min_gamma = 0.9, gamma=0.98, min_thresh=0.1, n_iter=1000, split_ratio=0.5:\n",
    "    - prec: 73.68\n",
    "    - spec: 94.68\n",
    "    - sens: 15.38\n",
    "8. N_sbt=3 (so far 5). min_gamma = 0.9, gamma=0.98, min_thresh=0.2, n_iter=1000, split_ratio=0.5:\n",
    "    - prec: 68.09\n",
    "    - spec: 92.02\n",
    "    - sens: 17.58\n",
    "9. Scale 12, N_sbt=3 (so far 5). min_gamma = 0.9, gamma=0.98, min_thresh=0.2, n_iter=1000, split_ratio=0.5:\n",
    "    - prec: 74.51\n",
    "    - spec: 93.09\n",
    "    - sens: 20.88\n",
    "10. Scale 12, N_sbt=5. min_gamma = 0.9, gamma=1, min_thresh=0.2, n_iter=1000, split_ratio=0.5:\n",
    "    - prec: 74.93\n",
    "    - spec: 95.67\n",
    "    - sens: 13.46\n",
    "11. Scale 12, N_sbt=3. min_gamma = 0.95, gamma=1, min_thresh=0.3, n_iter=500, split_ratio=0.5 (NO AGE):\n",
    "    - prec: 75\n",
    "    - spec: 94.68\n",
    "    - sens: 16.48\n",
    "12. Scale 12, N_sbt=4. min_gamma = 0.95, gamma=1, min_thresh=0.2, n_iter=500, split_ratio=0.5 (NO AGE):\n",
    "    - prec: 84.62\n",
    "    - spec: 97.87\n",
    "    - sens: 12.09\n",
    "13. Scale 12, N_sbt=4. min_gamma = 0.95, gamma=1, min_thresh=0.2, n_iter=500, split_ratio=0.5 (WITH AGE):\n",
    "    - prec: 91.30\n",
    "    - spec: 98.94\n",
    "    - sens: 11.54\n",
    "13. Scale 20, N_sbt=4. min_gamma = 0.95, gamma=1, min_thresh=0.2, n_iter=500, split_ratio=0.5 (WITH AGE):\n",
    "    - prec: 81.25\n",
    "    - spec: 98.40\n",
    "    - sens: 7.14"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36 (hbm)",
   "language": "python",
   "name": "hbm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
